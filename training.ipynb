{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckgp-vTPv_bg"
      },
      "outputs": [],
      "source": [
        "!pip install trl[peft] --quiet\n",
        "!pip install bitsandbytes loralib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcqf5l0xv_bi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, logging\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "from peft import LoraConfig\n",
        "import bitsandbytes as bnb\n",
        "logging.set_verbosity(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8h_HIF9v_bj"
      },
      "outputs": [],
      "source": [
        "with open(\"keywords.json\", 'r') as f:\n",
        "  keywords = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "8e2c916c3e22468ea7fe0b5ea857189e",
            "a8eb0f6a03464dfd96c4fe093096e8d0",
            "6f7512adc9e345ceb79cd45dc1bb0c32",
            "9551a05318224832b6b2c201bfd03eae",
            "6c2ca9d7d62a456b96ac1d98aef13bc6",
            "93166610b5e44ed5851d3d04c1a842fe",
            "f157b008cba74157ade711acb05d6b24",
            "a584f898b07c49c8827e376d194e30f4",
            "7168b800a4da4da88bbec75c5cc17194",
            "c1cad59e872b400daae7d67fc27d50ee",
            "53b9846aa6e9489e8bb3e650d9c52c82"
          ]
        },
        "id": "L7dDWtGZv_bn",
        "outputId": "0a49d1f2-9ea1-41e9-f898-4dfbabeaaef9"
      },
      "outputs": [],
      "source": [
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding=True, padding_side='left')\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['q_proj', 'v_proj']\n",
        ")\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id, peft_config = lora_config, load_in_4bit=True, bnb_4bit_quant_type='nf4')\n",
        "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_policy_response(template):\n",
        "    template+=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
        "    out_ids = model.generate(**inp_ids,max_new_tokens=20).squeeze()\n",
        "    start_gen = inp_ids.input_ids.shape[1]\n",
        "    out_ids = out_ids[start_gen:]\n",
        "    if id_eot in out_ids:\n",
        "        stop = out_ids.tolist().index(id_eot)\n",
        "        out = tokenizer.decode(out_ids[:stop])\n",
        "    else:\n",
        "        out = tokenizer.decode(out_ids)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCGJev0wv_bo"
      },
      "outputs": [],
      "source": [
        "class Environment():\n",
        "    def __init__(self) -> None:\n",
        "        self.questions = []\n",
        "        self.answers = []\n",
        "        self._select_keyword()\n",
        "\n",
        "        self.question_state = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
        "the user is going to think of a word, it can be only one of the following 3 categories:\n",
        "1. a place\n",
        "2. a person\n",
        "3. a thing\n",
        "So focus your area of search on these options. and give smart questions that narrows down the search space\\n\n",
        "your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
        "the user has chosen the word, start by asking your question!\n",
        "please be short and not verbose, output only your question, no extra word!<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "        self.guess_state = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
        "the user is going to think of a word, it can be only one of the following 3 categories:\n",
        "1. a place\n",
        "2. a person\n",
        "3. a thing\n",
        "So focus your area of search on these options. \\n\n",
        "based on the following conversation, can you guess the word, please give only the word, no verbosity around\\n<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    def _select_keyword(self):\n",
        "        category = np.random.choice(keywords)\n",
        "        chosen_key = np.random.choice(category['words'])\n",
        "        self.keyword = [chosen_key['keyword'].lower()]+[x.lower() for x in chosen_key['alts']]\n",
        "        # self.key_text = f\"\"\"{self.keyword[0]} {', also known as '+self.keyword[1] if len(self.keyword)>1 else''} {''.join([' and '+self.keyword[i] for i in self.keyword])}\"\"\"\n",
        "        self.category = category['category']\n",
        "\n",
        "\n",
        "    def step(self, question: str):\n",
        "        \"\"\"Given the question (action) the LLM (policy) will respond with yes or no.\n",
        "        using the same LLM that is being trained as policy to save memory.\"\"\"\n",
        "\n",
        "        self.questions.append(question)\n",
        "\n",
        "        # step 1 answer\n",
        "        sys_prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
        "        the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n",
        "        Know that the user will always guess a word belonging to one of the following 3 categories:\n",
        "        1. a place\n",
        "        2. a person\n",
        "        3. a thing\n",
        "        so make sure you understand the user's question and you understand the keyword you're playig on.\n",
        "        for now the word that the user should guess is: \"{self.keyword[0]}\" {', also known as '+self.keyword[1] if len(self.keyword)>1 else''} {''.join([' and '+x for x in self.keyword[2:]]) if len(self.keyword)>2 else ''},\n",
        "        it is of category \"{self.category}\",\n",
        "        to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n",
        "        example:\n",
        "        <user: is it a place?\n",
        "        you: yes\n",
        "        user: is it in europe?\n",
        "        you: no\n",
        "        user: is it in africa?\n",
        "        you: yes\n",
        "        user: do most people living there have dark skin?\n",
        "        you: no\n",
        "        user: is it a country name starting by m ?\n",
        "        you: yes\n",
        "        user: is it Morocco?\n",
        "        you: yes.>\"\"\"\n",
        "\n",
        "        chat_template = f\"\"\"<|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
        "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        chat_template += f\"{question}<|eot_id|>\"\n",
        "\n",
        "        output = generate_policy_response(chat_template)\n",
        "        self.answers.append(output)\n",
        "\n",
        "        # Step 2 Generate next state\n",
        "        self.question_state += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{question}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        self.question_state += f\"{output}<|eot_id|>\\n\"\n",
        "        # feed this state to policy to generate next question\n",
        "\n",
        "        # Step 3 Generate guess state\n",
        "        ## Experiment: Should we keep generating questions even thou we could guess correctly?\n",
        "        self.guess_state = self.guess_state[:-11] + f\"\"\"Question: {question}\\nAnswer: {output}\\n\"\"\" +\"\"\"<|eot_id|>\\n\"\"\"\n",
        "\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWxZNhtHv_bp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Wqo2Nvv_bp"
      },
      "outputs": [],
      "source": [
        "def data_generator(batch_size: int):\n",
        "\n",
        "    X = []\n",
        "    X_key_categories = [] # store ([keyword], category, no_of_qs, type: 'q'-> question, 'g'-> guess) that was used to create respective sample in X.\n",
        "    env = Environment()\n",
        "\n",
        "    for i in range(batch_size//2):\n",
        "        # Step 1 generate a question\n",
        "        # add from 0 to 19 of 20 questions in question state\n",
        "        X.append(env.question_state)\n",
        "        X_key_categories.append(json.dumps([env.keyword, env.category, len(env.questions)-1, 'q']))\n",
        "        generated_question = generate_policy_response(env.question_state)\n",
        "        for z in range(10):\n",
        "            if bool(generated_question.replace(' ', ''))==False:\n",
        "                generated_question = generate_policy_response(env.question_state)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # make new question and guess state\n",
        "        env.step(generated_question)\n",
        "        # Add from 1 to 20 questions here.\n",
        "        X.append(env.guess_state)\n",
        "\n",
        "        X_key_categories.append(json.dumps([env.keyword, env.category, len(env.questions), 'g']))\n",
        "\n",
        "        if len(env.questions) == 20:\n",
        "            env = Environment()\n",
        "\n",
        "    return X, X_key_categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fndPdEQLv_bp"
      },
      "outputs": [],
      "source": [
        "def reward(X, y, X_key_categories, r = [15.5, 15.25, 14.975, 14.672, 14.34, 13.974,\n",
        "                      13.571, 13.128, 12.641, 12.105, 11.516, 10.867,\n",
        "                      10.154, 9.369, 8.506, 7.557, 6.513, 5.364, 4.1, 2.71, 1.181]):\n",
        "    \"\"\"generates reward for two types of inputs\n",
        "\n",
        "    X_key_categories: ([keyword], category, no_of_qs, type: 'q'-> question, 'g'-> guess)\n",
        "\n",
        "    type = 'q': attaches the generated question (y) to respective X and use it to generate a guess from LLM policy.\n",
        "            If the guess is correct it generates a reward according to reward policy.\n",
        "    type = 'g': generates a reward if the guess is correct.\n",
        "\n",
        "    Reward policy:\n",
        "        r = a(b^x)+c, values used are a = -2.5, b = 1.1, c = 18\n",
        "    \"\"\"\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        X_key_categories[i] = json.loads(X_key_categories[i])\n",
        "        if X_key_categories[i][3]=='g':\n",
        "            if y[i].lower() in X_key_categories[i][0]:\n",
        "                rewards.append(torch.tensor(r[X_key_categories[i][2]]))\n",
        "            elif X_key_categories[i][2]==20:\n",
        "                rewards.append(torch.tensor(-7.5))\n",
        "            else:\n",
        "                rewards.append(torch.tensor(0.0))\n",
        "        else:\n",
        "            sys_prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
        "            the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n",
        "            Know that the user will always guess a word belonging to one of the following 3 categories:\n",
        "            1. a place\n",
        "            2. a person\n",
        "            3. a thing\n",
        "            so make sure you understand the user's question and you understand the keyword you're playig on.\n",
        "            for now the word that the user should guess is: \"{X_key_categories[i][0][0]}\" {', also known as '+X_key_categories[i][0][1] if len(X_key_categories[i][0])>1 else''} {''.join([' and '+m for m in X_key_categories[i][0][2:]]) if len(X_key_categories[i][0])>2 else ''},\n",
        "            it is of category \"{X_key_categories[i][1]}\",\n",
        "            to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n",
        "            example:\n",
        "            <user: is it a place?\n",
        "            you: yes\n",
        "            user: is it in europe?\n",
        "            you: no\n",
        "            user: is it in africa?\n",
        "            you: yes\n",
        "            user: do most people living there have dark skin?\n",
        "            you: no\n",
        "            user: is it a country name starting by m ?\n",
        "            you: yes\n",
        "            user: is it Morocco?\n",
        "            you: yes.>\"\"\"\n",
        "\n",
        "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
        "            chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            chat_template += f\"{y[i]}<|eot_id|>\"\n",
        "\n",
        "            output = generate_policy_response(chat_template)\n",
        "\n",
        "            prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "            You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
        "            the user is going to think of a word, it can be only one of the following 3 categories:\n",
        "            1. a place\n",
        "            2. a person\n",
        "            3. a thing\n",
        "            So focus your area of search on these options. \\n\n",
        "            based on the following conversation, can you guess the word, please give only the word, no verbosity around\n",
        "            \"\"\"\n",
        "            # Extracting previous questions from X\n",
        "            for text in X[i].split('<|start_header_id|>assistant<|end_header_id|>')[1:-1]:\n",
        "                q,a = text.split('<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\n')\n",
        "                q = q.lstrip().replace('\\n', '')\n",
        "                a = a.replace('\\n', '').replace('<|eot_id|>', '')\n",
        "                prompt+=f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
        "\n",
        "            prompt+=f\"\"\"Question: {y[i]}\\nAnswer: {output}\\n\"\"\"\n",
        "\n",
        "            guess_word = generate_policy_response(prompt)\n",
        "            if guess_word.lower() in X_key_categories[i][0]:\n",
        "                rewards.append(torch.tensor(r[X_key_categories[i][2]]))\n",
        "            elif X_key_categories[i][2]>=19:\n",
        "                rewards.append(torch.tensor(-7.5))\n",
        "            else:\n",
        "                rewards.append(torch.tensor(0.0))\n",
        "\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arx-sHFUv_bq"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, num_records=512):\n",
        "        super().__init__()\n",
        "        x, metadata = data_generator(num_records)\n",
        "        input_ids = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
        "        bos_token = torch.zeros((input_ids.shape[0],1), dtype=torch.int64)+tokenizer.bos_token_id\n",
        "        assistant_id = tokenizer(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", return_tensors=\"pt\")['input_ids'].squeeze()[1:]\n",
        "        assistant_id_tensor = torch.zeros((input_ids.shape[0], assistant_id.shape[0]), dtype=torch.int64)+assistant_id\n",
        "\n",
        "        dataset = datasets.Dataset.from_dict({'query': x, 'metadata': metadata,\n",
        "                                             'input_ids': torch.concat((bos_token, input_ids, assistant_id_tensor), dim=1)})\n",
        "        del input_ids, assistant_id, assistant_id_tensor\n",
        "\n",
        "\n",
        "        def tokenize_this(sample):\n",
        "            sample['input_ids'] = tokenizer.encode(sample['query'])\n",
        "            return sample\n",
        "\n",
        "        dataset.set_format(type=\"torch\")\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dV0ByVCev_bs",
        "outputId": "bebe6710-cfcf-465c-a9e7-01ec9cf59dd3"
      },
      "outputs": [],
      "source": [
        "# PPO Trainer\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_id,\n",
        "    learning_rate=1.41e-5,\n",
        "    batch_size=8,\n",
        "    mini_batch_size=1,\n",
        "    optimize_device_cache=True,\n",
        ")\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"max_new_tokens\": 30,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "epochs = 10\n",
        "stats = []\n",
        "\n",
        "for i in tqdm(range(epochs), \"epoch: \"):\n",
        "    epoch_reward = 0\n",
        "    dataset = DataGenerator(100)\n",
        "\n",
        "    ppo_trainer = PPOTrainer(\n",
        "        model=model,\n",
        "        config=config,\n",
        "        dataset=dataset,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    device = ppo_trainer.accelerator.device\n",
        "    if ppo_trainer.accelerator.num_processes == 1:\n",
        "        device = 0 if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for batch in tqdm(ppo_trainer.dataloader):\n",
        "\n",
        "        query_tensors = [batch['input_ids'][i,:].to(\"cuda\") for i in range(batch['input_ids'].shape[0])]\n",
        "\n",
        "        #### Get response from SFTModel\n",
        "        # response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
        "        # batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "        response_tensors = [ppo_trainer.generate(x, **generation_kwargs).squeeze() for x in query_tensors]\n",
        "        start_gen = batch['input_ids'].shape[1]\n",
        "        batch['response'] = []\n",
        "        for x in response_tensors:\n",
        "            out_ids = x[start_gen:]\n",
        "            if id_eot in out_ids:\n",
        "                stop = out_ids.tolist().index(id_eot)\n",
        "                out = tokenizer.decode(out_ids[:stop])\n",
        "            else:\n",
        "                out = tokenizer.decode(out_ids)\n",
        "            batch[\"response\"].append(out)\n",
        "\n",
        "        #### Compute reward score\n",
        "        rewards = reward(batch['query'], batch['response'], batch['metadata'])\n",
        "\n",
        "        #### Run PPO step\n",
        "        st = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "        # ppo_trainer.log_stats(stats, batch=[], rewards=rewards)\n",
        "        stats.append({\"epoch\": i,\n",
        "                      \"stats\": st,\n",
        "                      \"rewards\": rewards,\n",
        "                      \"batch_query\": batch['query'],\n",
        "                      \"batch_response\": batch['response']})\n",
        "        epoch_reward += sum(rewards)\n",
        "    print(epoch_reward)\n",
        "\n",
        "#### Save model\n",
        "ppo_trainer.save_pretrained(\"my_ppo_model_v2\")\n",
        "# model.save_pretrained(\"my_ppo_model_v2_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zukgxcbHzcWl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 8550470,
          "sourceId": 61247,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30747,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53b9846aa6e9489e8bb3e650d9c52c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c2ca9d7d62a456b96ac1d98aef13bc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f7512adc9e345ceb79cd45dc1bb0c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a584f898b07c49c8827e376d194e30f4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7168b800a4da4da88bbec75c5cc17194",
            "value": 4
          }
        },
        "7168b800a4da4da88bbec75c5cc17194": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e2c916c3e22468ea7fe0b5ea857189e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8eb0f6a03464dfd96c4fe093096e8d0",
              "IPY_MODEL_6f7512adc9e345ceb79cd45dc1bb0c32",
              "IPY_MODEL_9551a05318224832b6b2c201bfd03eae"
            ],
            "layout": "IPY_MODEL_6c2ca9d7d62a456b96ac1d98aef13bc6"
          }
        },
        "93166610b5e44ed5851d3d04c1a842fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9551a05318224832b6b2c201bfd03eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1cad59e872b400daae7d67fc27d50ee",
            "placeholder": "​",
            "style": "IPY_MODEL_53b9846aa6e9489e8bb3e650d9c52c82",
            "value": " 4/4 [00:11&lt;00:00,  2.54s/it]"
          }
        },
        "a584f898b07c49c8827e376d194e30f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8eb0f6a03464dfd96c4fe093096e8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93166610b5e44ed5851d3d04c1a842fe",
            "placeholder": "​",
            "style": "IPY_MODEL_f157b008cba74157ade711acb05d6b24",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c1cad59e872b400daae7d67fc27d50ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f157b008cba74157ade711acb05d6b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
